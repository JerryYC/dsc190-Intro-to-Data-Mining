{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    \n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "        n_features: Number of features to use during building the tree.(Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
    "        \"\"\"\n",
    "        \n",
    "            Initializations for class attributes.\n",
    "            \n",
    "            TODO: 1. Modify the initialization of the attributes of the Decision Tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = None             \n",
    "        self.max_depth = max_depth        \n",
    "        self.size_allowed = size_allowed      \n",
    "        self.n_features = n_features       \n",
    "        self.n_split = n_split           \n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        \"\"\"\n",
    "            Node Class for the building the tree.\n",
    "\n",
    "            Attribute: \n",
    "                threshold: The threshold like if x1 < threshold, for spliting.\n",
    "                feature: The index of feature on this current node.\n",
    "                left: Pointer to the node on the left.\n",
    "                right: Pointer to the node on the right.\n",
    "                pure: Bool, describe if this node is pure.\n",
    "                predict: Class, indicate what the most common Y on this node.\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, threshold = None, feature = None):\n",
    "            \"\"\"\n",
    "            \n",
    "                Initializations for class attributes.\n",
    "                \n",
    "                TODO: 2. Modify the initialization of the attributes of the Node. (Initialize threshold and feature)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            self.threshold = threshold   \n",
    "            self.feature = feature    \n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = False\n",
    "            self.depth = None\n",
    "            self.predict = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, lst):\n",
    "        \"\"\"\n",
    "            Function Calculate the entropy given lst.\n",
    "            \n",
    "            Attributes: \n",
    "                entro: variable store entropy for each step.\n",
    "                classes: all possible classes. (without repeating terms)\n",
    "                counts: counts of each possible classes.\n",
    "                total_counts: number of instances in this lst.\n",
    "                \n",
    "            lst is vector of labels.\n",
    "                \n",
    "            \n",
    "            \n",
    "            TODO: 3. Intilize attributes.\n",
    "                  4. Modify and add some codes to the following for-loop\n",
    "                     to compute the correct entropy. \n",
    "                     (make sure count of corresponding label is not 0, think about why we need to do that.)\n",
    "        \"\"\"\n",
    "        \n",
    "        _, counts = np.unique(lst, return_counts=True) \n",
    "        counts = counts / counts.sum()\n",
    "        return -(counts * np.log(counts)).sum()\n",
    "\n",
    "    def information_gain(self, lst, values, threshold):\n",
    "        \"\"\"\n",
    "        \n",
    "            Function Calculate the information gain, by using entropy function.\n",
    "            \n",
    "            lst is vector of labels.\n",
    "            values is vector of values for individule feature.\n",
    "            threshold is the split threshold we want to use for calculating the entropy.\n",
    "            \n",
    "            \n",
    "            TODO:\n",
    "                5. Modify the following variable to calculate the P(left node), P(right node), \n",
    "                   Conditional Entropy(left node) and Conditional Entropy(right node)\n",
    "                6. Return information gain.\n",
    "                \n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        idx = (values < threshold)\n",
    "        left_prop = idx.mean()  \n",
    "        right_prop = 1 - left_prop\n",
    "        \n",
    "        total_entropy = self.entropy(lst)\n",
    "        left_entropy = self.entropy(lst[idx])\n",
    "        right_entropy = self.entropy(lst[np.invert(idx)])\n",
    "        \n",
    "        return total_entropy - left_entropy * left_prop - right_entropy * right_prop \n",
    "    \n",
    "    def find_rules(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            Helper function to find the split rules.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            TODO: 7. Modify the following for loop, which loop through each column(feature).\n",
    "                     Find the unique value of each feature, and find the mid point of each adjacent value.\n",
    "                  8. Store them in a list, return that list.\n",
    "            \n",
    "        \"\"\"      \n",
    "        rules = []        \n",
    "        for col in data.T:          \n",
    "            unique_value = np.unique(col)\n",
    "            diff  = unique_value[:-1] + np.diff(unique_value)/2      \n",
    "            rules.append(diff)             \n",
    "        return rules\n",
    "    \n",
    "    def next_split(self, data, label):\n",
    "        \"\"\"\n",
    "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "            \n",
    "            TODO: 9. Use find_rules to initialize rules variable\n",
    "                  10. Initialize max_info to some negative number.\n",
    "        \"\"\"\n",
    "        \n",
    "        rules = self.find_rules(data)             \n",
    "        max_info = -2          \n",
    "        num_col = None          \n",
    "        threshold = None       \n",
    "        entropy_y = self.entropy(label)      \n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "            TODO: 11. Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
    "                      If n_features is a int, use n_features of features by random choice. \n",
    "                      If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
    "                      \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.n_features is None :\n",
    "            index_col = np.arange(data.shape[1])\n",
    "        else:\n",
    "            if isinstance(self.n_features, int): \n",
    "                num_index = n_features \n",
    "            elif self.n_features == 'sqrt':\n",
    "                num_index = int(np.sqrt(data.shape[1]))\n",
    "            np.random.seed()  \n",
    "            index_col = np.random.choice(data.shape[1], num_index, replace = False)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            TODO: 12. Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
    "                  13. For all selected feature and corresponding rules, we check it's information gain. \n",
    "                  \n",
    "        \"\"\"\n",
    "        for i in index_col:\n",
    "            count_temp_rules = len(rules[i])\n",
    "            \n",
    "            if self.n_split is None:\n",
    "                index_rules = np.arange(count_temp_rules)\n",
    "            else:\n",
    "                if isinstance(self.n_split, int):\n",
    "                    num_rules = self.n_split    \n",
    "                elif self.n_split == 'sqrt':\n",
    "                    num_rules = int(np.sqrt(count_temp_rules))\n",
    "                np.random.seed()\n",
    "                index_rules = np.random.choice(count_temp_rules, num_rules, replace = False)\n",
    "\n",
    "            for j in index_rules:\n",
    "                info = self.information_gain(lst = label, values = data[:,i], threshold = rules[i][j])   \n",
    "                if info > max_info:  \n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rules[i][j]\n",
    "        return threshold, num_col\n",
    "        \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \n",
    "            \"\"\"\n",
    "                Helper function for building the tree.\n",
    "                \n",
    "                TODO: 14. First build the root node.\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            first_threshold, first_feature = self.next_split(data = X, label = y)\n",
    "            current = self.Node(first_threshold, first_feature)  \n",
    "            current.depth = depth\n",
    "            \"\"\"\n",
    "                TODO: 15. Base Case 1: Check if we pass the max_depth, check if the first_feature is None, min split size.\n",
    "                          If some of those condition met, change current to pure, and set predict to the most popular label\n",
    "                          and return current\n",
    "                          \n",
    "                \n",
    "            \"\"\"\n",
    "            if (first_feature is None) or (self.max_depth != None and depth >= self.max_depth) \\\n",
    "                or (self.size_allowed != None and X.shape[0] < self.size_allowed):\n",
    "                labels, cnts = np.unique(y,return_counts = True)\n",
    "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "               Base Case 2: Check if there is only 1 label in this node, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(np.unique(y)) == 1:\n",
    "                labels, cnts = np.unique(y,return_counts = True)\n",
    "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 16. Find the left node index with feature i <= threshold  Right with feature i > threshold.\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            \n",
    "            left_index = X[:,first_feature] < first_threshold\n",
    "            right_index = np.invert(left_index)\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 17. Base Case 3: If we either side is empty, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            if left_index.sum() == 0 or right_index.sum() == 0 :\n",
    "                labels, cnts = np.unique(y,return_counts = True)\n",
    "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
    "                current.pure = True \n",
    "                return current\n",
    "            \n",
    "            \n",
    "            left_X, left_y = X[left_index,:], y[left_index]\n",
    "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "                \n",
    "            right_X, right_y = X[right_index,:], y[right_index]\n",
    "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "            \n",
    "            return current\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Decision Tree model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "\n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 1)\n",
    "        \n",
    "\n",
    "        self.for_runing = y[0]\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "            \n",
    "            TODO: 18. Modify the following while loop to get the prediction.\n",
    "                      Stop condition we are at a node is pure.\n",
    "                      Trace with the threshold and feature.\n",
    "                19. Change return self.for_runing to appropiate value.\n",
    "        \"\"\"\n",
    "        cur = self.root  \n",
    "        while not cur.pure:  \n",
    "            \n",
    "            feature = cur.feature\n",
    "            threshold = cur.threshold\n",
    "            \n",
    "            if inp[feature] < threshold:  \n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.predict\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 20. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umUfc-Kj-gGv"
   },
   "outputs": [],
   "source": [
    "X = np.array(wine)[:, :-1]\n",
    "y = np.array(wine)[:, -1]\n",
    "y = np.array(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p2aQgpE-gGz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eACyWCM9-gG1"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\"entropy\")\n",
    "tree = tree.fit(X_train, y_train)\n",
    "clf = DecisionTree()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Error should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### My Model\n",
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sklearn Model\n",
    "pred = tree.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Error should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.628125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### My Model\n",
    "pred = clf.predict(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.609375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sklearn Model\n",
    "pred = tree.predict(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RandomForest Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        n_trees: Number of trees. \n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = None, size_allowed = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initilize all Attributes.\n",
    "            \n",
    "            TODO: 1. Intialize n_trees, n_features, n_split, max_depth, size_allowed.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Random Forest model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "            \n",
    "        \n",
    "            TODO: 2. Modify the following for loop to create n_trees tress. by calling DecisionTree we created.\n",
    "                     Pass in all the attributes.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_trees):\n",
    "            np.random.seed()\n",
    "            temp_clf = DecisionTree(max_depth = self.max_depth, size_allowed = self.size_allowed,\n",
    "                                    n_features = self.n_features, n_split = self.n_split)\n",
    "            temp_clf.fit(X, y)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \n",
    "            TODO: 4. Modify the following code to predict using each Decision Tree.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for tree in self.trees:\n",
    "            result.append(tree.ind_predict(inp))\n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "            TODO: 5. Modify the following code to find the correct prediction use majority rule.\n",
    "                     If there is a tie, use random choice to select one of them.\n",
    "        \"\"\"\n",
    "        labels, counts = np.unique(result, return_counts = True)\n",
    "        return np.random.choice(labels[counts == max(counts)])\n",
    "    \n",
    "    def predict_all(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 6. Revise the following for-loop to call ind_predict to get predictions\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for row in inp:\n",
    "            result.append(self.ind_predict(row))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [],
   "source": [
    "clf = RandomForest(n_trees= 100)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### My Model - Train Accuracy\n",
    "pred = clf.predict_all(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### My Model - Test Accuracy\n",
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sklearn Model - Train\n",
    "skclf = RandomForestClassifier(criterion='entropy')\n",
    "skclf = skclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sklearn Model - Train Accuracy\n",
    "pred = skclf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.709375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sklearn Model - Test Accuracy\n",
    "pred = skclf.predict(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = []\n",
    "train_accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,10,100,1000]:\n",
    "    clf = RandomForest(n_trees= i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict_all(X_test)\n",
    "    test_accs.append((pred == y_test).mean())\n",
    "    pred = clf.predict_all(X_train)\n",
    "    train_accs.append((pred == y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "ax = f.subplots()\n",
    "# plot training and validation accuracy\n",
    "ax.plot(train_accs, label = \"Train accuracy\")\n",
    "ax.plot(test_accs, label = \"Test accuracy\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel(\"Number of Trees (log)\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
