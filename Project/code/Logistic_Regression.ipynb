{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    \"\"\"\n",
    "        Helper function for movielens dataset, not useful for discrete multi class clasification.\n",
    "\n",
    "        Return:\n",
    "        Normalized list x, in range [0, 1]\n",
    "    \"\"\"\n",
    "    maximum = max(lst)\n",
    "    minimum = min(lst)\n",
    "    toreturn = []\n",
    "    for i in range(len(lst)):\n",
    "        toreturn.append((lst[i]- minimum)/ (maximum - minimum))\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_standardize(X_inp):\n",
    "    \"\"\"\n",
    "        Z-score Standardization.\n",
    "        Standardize the feature matrix, and store the standarize rule.\n",
    "\n",
    "        Parameter:\n",
    "        X_inp: Input feature matrix.\n",
    "\n",
    "        Return:\n",
    "        Standardized feature matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    toreturn = X_inp.copy()\n",
    "    for i in range(X_inp.shape[1]):\n",
    "        std = np.std(X_inp[:, i])               # ------ Find the standard deviation of the feature\n",
    "        mean = np.mean(X_inp[:, i])             # ------ Find the mean value of the feature\n",
    "        toreturn[:, i] = (X_inp[:, i] - mean)/std\n",
    "    return toreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" \n",
    "        Sigmoid Function\n",
    "\n",
    "        Return:\n",
    "        transformed x.\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "        #TODO: 2. implement the sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy(y, pred):\n",
    "    return (-y * np.log(pred) - (1 - y) * np.log(1 - pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Logistic Regression'\n",
    "    \n",
    "    def fit(self, X_train, y_train, standardized = True):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and do normalization to y_train\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "        count = 0\n",
    "        uni = np.unique(y_train)\n",
    "        for i,y in enumerate(y_train):\n",
    "            if y == min(uni):\n",
    "                self.y[i] = 0\n",
    "            else:\n",
    "                self.y[i] = 1 \n",
    "        self.standardized = standardized\n",
    "        if self.standardized:\n",
    "            self.X = z_standardize(self.X)\n",
    "        self.W = np.zeros(X_train.shape[1])\n",
    "        self.b = 0\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"\n",
    "            Calculate the grandient of Weight and Bias\n",
    "        \"\"\"\n",
    "        Z = X @ self.W + self.b\n",
    "        \n",
    "        y_hat = sigmoid(Z)\n",
    "        dy = (y_hat - y) / (self.X.shape[0])\n",
    "        dZ = dy * sigmoid(y_hat) * (1-sigmoid(y_hat))\n",
    "        dW = X.T @ dZ\n",
    "        db = np.ones((1, X.shape[0])) @ dZ \n",
    "        return dW, db\n",
    "\n",
    "    def gradient_descent_logistic(self, alpha, num_pass, early_stop=0):\n",
    "        \"\"\"\n",
    "            Logistic Regression with gradient descent method\n",
    "\n",
    "            Parameter:\n",
    "                alpha: (Hyper Parameter) Learning rate.\n",
    "                num_pass: Number of iteration\n",
    "                early_stop: (Hyper Parameter) Least improvement error allowed before stop. \n",
    "                            If improvement is less than the given value, then terminate the function and store the coefficents.\n",
    "                            default = 0.\n",
    "                standardized: bool, determine if we standardize the feature matrix.\n",
    "                \n",
    "            Return:\n",
    "                self.theta: theta after training\n",
    "                self.b: b after training\n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        for i in range(num_pass):    \n",
    "            dW, db = self.gradient(self.X, self.y)\n",
    "            self.W = self.W - alpha * dW\n",
    "            self.b = self.b - alpha * db\n",
    "            \n",
    "            y_hat = sigmoid(self.X @ self.W + self.b)\n",
    "            loss = cross_entropy(self.y, y_hat)\n",
    "\n",
    "            if (len(self.loss) > 0 ) and ((abs(self.loss[-1] - loss) < early_stop) |\n",
    "                                            (abs(abs(self.loss[-1] - loss) / self.loss[-1]) < early_stop)):\n",
    "                return self.W, self.b\n",
    "\n",
    "            self.loss += [loss]\n",
    "            self.acc += [((self.y == (y_hat >= 0.5)).mean())]\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('Iteration: ' +  str(i))\n",
    "                print('Coef: '+ str(self.W) + \", \" + str(self.b))\n",
    "                print('Loss: ' + str(loss))   \n",
    "        return self.W, self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "        \"\"\"\n",
    "        if self.standardized:\n",
    "            self.X = z_standardize(self.X)\n",
    "        return np.where(sigmoid(X @ self.W + self.b) >= 0.5, 1, 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "#names = ['f_acid', 'v_acid', 'c_acid', 'sugar', 'chlorides', 'f_SO2', 't_SO2', 'density', 'ph', 'sulphates', 'alcohol', 'quality']\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1319 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "5               7.4              0.66         0.00             1.8      0.075   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1592            6.3              0.51         0.13             2.3      0.076   \n",
       "1593            6.8              0.62         0.08             1.9      0.068   \n",
       "1595            5.9              0.55         0.10             2.2      0.062   \n",
       "1596            6.3              0.51         0.13             2.3      0.076   \n",
       "1598            6.0              0.31         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "4         9.4        5  \n",
       "5         9.4        5  \n",
       "...       ...      ...  \n",
       "1592     11.0        6  \n",
       "1593      9.5        6  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1319 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine5 = wine.loc[wine.quality == 5]\n",
    "wine6 = wine.loc[wine.quality == 6]\n",
    "wineall = pd.concat([wine5,wine6])\n",
    "wineall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(wineall.iloc[:,:10])\n",
    "Y = np.array(wineall.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for y in Y:\n",
    "    if y == 5:\n",
    "        Y[count] = 0\n",
    "    else:\n",
    "        Y[count] = 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Logistic_Regression()\n",
    "logit.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [ 6.27683713e-05 -2.78558358e-04  9.41231922e-05 -2.16700614e-05\n",
      " -9.60805618e-05 -7.11897427e-05 -2.80758301e-04 -1.58025055e-04\n",
      "  5.05755007e-05  1.90727427e-04], [-3.83061396e-05]\n",
      "Loss: 0.6930411107082884\n",
      "Iteration: 1000\n",
      "Coef: [ 0.04594997 -0.19108238  0.05105867  0.00900075 -0.0781566  -0.021157\n",
      " -0.2033459  -0.12871574  0.05089631  0.14822019], [-0.0324577]\n",
      "Loss: 0.6350972084140096\n",
      "Iteration: 2000\n",
      "Coef: [ 0.0834463  -0.28122899  0.06165878  0.04354911 -0.12845384  0.00117229\n",
      " -0.31910125 -0.21472579  0.08987958  0.24298043], [-0.05313458]\n",
      "Loss: 0.6153240026009489\n",
      "Iteration: 3000\n",
      "Coef: [ 0.12152665 -0.32677755  0.05917715  0.08014262 -0.16159769  0.03192071\n",
      " -0.39652544 -0.2774166   0.11723673  0.30898524], [-0.06592489]\n",
      "Loss: 0.6057961783675673\n",
      "Iteration: 4000\n",
      "Coef: [ 0.15983953 -0.35018831  0.05186447  0.11254042 -0.18401809  0.06193623\n",
      " -0.45332685 -0.32684761  0.13708584  0.35712621], [-0.07390404]\n",
      "Loss: 0.6002174361400108\n",
      "Iteration: 5000\n",
      "Coef: [ 0.19738479 -0.3619163   0.04289736  0.13965669 -0.1994485   0.08889333\n",
      " -0.49713122 -0.36803672  0.15238628  0.39329476], [-0.07896426]\n",
      "Loss: 0.5965555689044912\n",
      "Iteration: 6000\n",
      "Coef: [ 0.23350669 -0.36731538  0.03365116  0.16196327 -0.21013147  0.11243339\n",
      " -0.5318366  -0.40359828  0.16496813  0.42106342], [-0.08223395]\n",
      "Loss: 0.5939735691679242\n",
      "Iteration: 7000\n",
      "Coef: [ 0.26786924 -0.36927844  0.02472493  0.18030168 -0.21747833  0.13276946\n",
      " -0.55974436 -0.43499058  0.17592797  0.44274693], [-0.08438372]\n",
      "Loss: 0.5920634481265866\n",
      "Iteration: 8000\n",
      "Coef: [ 0.30034228 -0.36941727  0.01635242  0.19548826 -0.22241863  0.15025308\n",
      " -0.58237078 -0.46309539  0.18591265  0.45991275], [-0.08581513]\n",
      "Loss: 0.5906009166648734\n",
      "Iteration: 9000\n",
      "Coef: [ 0.33091644 -0.36864471  0.00859236  0.20820387 -0.22558927  0.16524042\n",
      " -0.60079319 -0.48849237  0.19529586  0.47365816], [-0.08677218]\n",
      "Loss: 0.5894514159046288\n",
      "Iteration: 10000\n",
      "Coef: [ 0.35964938 -0.36748031  0.00142256  0.21898385 -0.22744109  0.17805522\n",
      " -0.61581543 -0.51159303  0.20428613  0.48477206], [-0.08740626]\n",
      "Loss: 0.5885288734061683\n",
      "Iteration: 11000\n",
      "Coef: [ 0.38663323 -0.36621757 -0.00521268  0.22823882 -0.22830167  0.18898236\n",
      " -0.62805663 -0.53270861  0.21299326  0.49383425], [-0.08781366]\n",
      "Loss: 0.5877755452273321\n",
      "Iteration: 12000\n",
      "Coef: [ 0.41197542 -0.36501822 -0.01137892  0.23628102 -0.22841399  0.19827029\n",
      " -0.63800408 -0.55208603  0.22146923  0.50127879], [-0.08805745]\n",
      "Loss: 0.5871512680338562\n",
      "Iteration: 13000\n",
      "Coef: [ 0.43578756 -0.36396663 -0.01714014  0.24334779 -0.22796122  0.20613531\n",
      " -0.64604752 -0.56992788  0.22973334  0.50743574], [-0.08818023]\n",
      "Loss: 0.5866273088051751\n",
      "Iteration: 14000\n",
      "Coef: [ 0.45817916 -0.36310165 -0.02255386  0.24962044 -0.22708323  0.21276584\n",
      " -0.65250259 -0.58640428  0.23778755  0.5125594 ], [-0.08821171]\n",
      "Loss: 0.5861826391787024\n",
      "Iteration: 15000\n",
      "Coef: [ 0.47925438 -0.36243557 -0.02766942  0.25523849 -0.22588778  0.21832607\n",
      " -0.6576276  -0.60166022  0.2456256   0.51684786], [-0.08817326]\n",
      "Loss: 0.5858015736489831\n",
      "Iteration: 16000\n",
      "Coef: [ 0.49911046 -0.36196546 -0.03252795  0.26031036 -0.22445854  0.2229591\n",
      " -0.66163575 -0.61582068  0.25323838  0.52045699], [-0.08808067]\n",
      "Loss: 0.5854722173323511\n",
      "Iteration: 17000\n",
      "Coef: [ 0.51783717 -0.36168007 -0.03716308  0.26492113 -0.22286078  0.2267897\n",
      " -0.66470442 -0.62899409  0.26061689  0.52351039], [-0.08794596]\n",
      "Loss: 0.5851854160059895\n",
      "Iteration: 18000\n",
      "Coef: [ 0.5355168  -0.36156387 -0.04160195  0.26913834 -0.22114555  0.2299266\n",
      " -0.66698204 -0.64127511  0.26775375  0.52610682], [-0.08777843]\n",
      "Loss: 0.5849340291828569\n",
      "Iteration: 19000\n",
      "Coef: [ 0.55222454 -0.36159961 -0.04586621  0.27301617 -0.21935287  0.23246456\n",
      " -0.66859353 -0.65274674  0.27464382  0.52832572], [-0.08758544]\n",
      "Loss: 0.5847124170956648\n",
      "Iteration: 20000\n",
      "Coef: [ 0.56802896 -0.36176973 -0.04997304  0.27659856 -0.21751409  0.23448612\n",
      " -0.66964449 -0.66348199  0.28128442  0.53023128], [-0.08737288]\n",
      "Loss: 0.5845160727388314\n",
      "Iteration: 21000\n",
      "Coef: [ 0.58299255 -0.36205722 -0.053936    0.27992155 -0.21565374  0.23606318\n",
      " -0.67022449 -0.67354536  0.28767511  0.53187572], [-0.08714555]\n",
      "Loss: 0.5843413542324136\n",
      "Iteration: 22000\n",
      "Coef: [ 0.59717225 -0.3624461  -0.05776577  0.28301495 -0.21379092  0.23725837\n",
      " -0.67040977 -0.68299397  0.29381752  0.53330164], [-0.08690735]\n",
      "Loss: 0.5841852877271385\n",
      "Iteration: 23000\n",
      "Coef: [ 0.61062006 -0.36292161 -0.06147077  0.28590368 -0.21194046  0.2381262\n",
      " -0.67026533 -0.69187859  0.29971497  0.534544  ], [-0.0866615]\n",
      "Loss: 0.584045420621223\n",
      "Iteration: 24000\n",
      "Coef: [ 0.62338346 -0.3634703  -0.06505766  0.28860879 -0.21011377  0.23871411\n",
      " -0.66984669 -0.70024445  0.30537217  0.53563156], [-0.08641067]\n",
      "Loss: 0.5839197111117974\n",
      "Iteration: 25000\n",
      "Coef: [ 0.63550592 -0.36408008 -0.06853175  0.29114813 -0.20831951  0.23906337\n",
      " -0.66920137 -0.70813202  0.31079492  0.5365881 ], [-0.08615707]\n",
      "Loss: 0.5838064442812901\n",
      "Iteration: 26000\n",
      "Coef: [ 0.64702729 -0.36474008 -0.07189733  0.29353705 -0.2065642   0.23920991\n",
      " -0.66837    -0.71557755  0.31598984  0.53743331], [-0.08590253]\n",
      "Loss: 0.5837041677595797\n",
      "Iteration: 27000\n",
      "Coef: [ 0.65798417 -0.36544065 -0.07515794  0.29578878 -0.2048526   0.23918493\n",
      " -0.66738741 -0.72261366  0.32096413  0.5381836 ], [-0.08564855]\n",
      "Loss: 0.5836116419632286\n",
      "Iteration: 28000\n",
      "Coef: [ 0.66841025 -0.36617324 -0.07831655  0.29791482 -0.20318811  0.23901555\n",
      " -0.66628338 -0.72926975  0.32572539  0.53885263], [-0.08539638]\n",
      "Loss: 0.5835278012841645\n",
      "Iteration: 29000\n",
      "Coef: [ 0.67833658 -0.3669303  -0.08137575  0.29992525 -0.20157302  0.23872532\n",
      " -0.6650834  -0.73557242  0.33028146  0.53945186], [-0.08514706]\n",
      "Loss: 0.5834517235702754\n",
      "Iteration: 30000\n",
      "Coef: [ 0.68779187 -0.36770522 -0.08433784  0.30182894 -0.20000875  0.23833468\n",
      " -0.66380927 -0.74154578  0.33464025  0.5399909 ], [-0.08490142]\n",
      "Loss: 0.583382605934092\n",
      "Iteration: 31000\n",
      "Coef: [ 0.69680267 -0.36849218 -0.08720494  0.30363375 -0.19849603  0.23786132\n",
      " -0.66247958 -0.74721174  0.3388097   0.54047783], [-0.08466013]\n",
      "Loss: 0.5833197454265892\n",
      "Iteration: 32000\n",
      "Coef: [ 0.7053936  -0.36928614 -0.08997904  0.30534666 -0.19703504  0.23732058\n",
      " -0.66111017 -0.75259028  0.34279765  0.5409195 ], [-0.08442374]\n",
      "Loss: 0.5832625234780162\n",
      "Iteration: 33000\n",
      "Coef: [ 0.71358754 -0.37008271 -0.0926621   0.30697394 -0.19562555  0.2367257\n",
      " -0.65971452 -0.75769965  0.34661181  0.54132168], [-0.08419267]\n",
      "Loss: 0.5832103932756755\n",
      "Iteration: 34000\n",
      "Coef: [ 0.72140578 -0.37087809 -0.09525601  0.30852122 -0.19426697  0.23608813\n",
      " -0.65830404 -0.76255658  0.35025968  0.54168929], [-0.08396724]\n",
      "Loss: 0.5831628694469483\n",
      "Iteration: 35000\n",
      "Coef: [ 0.72886815 -0.37166902 -0.09776268  0.30999358 -0.19295849  0.23541771\n",
      " -0.65688838 -0.76717639  0.35374857  0.54202652], [-0.0837477]\n",
      "Loss: 0.5831195195637737\n",
      "Iteration: 36000\n",
      "Coef: [ 0.73599318 -0.37245273 -0.10018401  0.31139565 -0.19169907  0.23472292\n",
      " -0.65547565 -0.77157321  0.35708552  0.54233693], [-0.08353422]\n",
      "Loss: 0.5830799570957981\n",
      "Iteration: 37000\n",
      "Coef: [ 0.74279817 -0.37322683 -0.10252192  0.31273162 -0.19048756  0.23401103\n",
      " -0.65407263 -0.77576006  0.36027732  0.5426236 ], [-0.08332691]\n",
      "Loss: 0.583043835523262\n",
      "Iteration: 38000\n",
      "Coef: [ 0.74929935 -0.37398934 -0.10477838  0.31400535 -0.18932268  0.23328824\n",
      " -0.65268497 -0.77974895  0.36333049  0.54288915], [-0.08312583]\n",
      "Loss: 0.5830108433844053\n",
      "Iteration: 39000\n",
      "Coef: [ 0.7555119  -0.37473859 -0.10695536  0.31522035 -0.1882031   0.23255985\n",
      " -0.65131732 -0.78355102  0.36625128  0.54313586], [-0.082931]\n",
      "Loss: 0.5829807000808516\n",
      "Iteration: 40000\n",
      "Coef: [ 0.76145009 -0.3754732  -0.10905485  0.31637989 -0.18712743  0.23183035\n",
      " -0.6499735  -0.78717658  0.36904568  0.54336567], [-0.0827424]\n",
      "Loss: 0.5829531523018241\n",
      "Iteration: 41000\n",
      "Coef: [ 0.76712734 -0.37619207 -0.11107887  0.31748695 -0.18609424  0.23110354\n",
      " -0.64865658 -0.79063523  0.37171941  0.5435803 ], [-0.08255999]\n",
      "Loss: 0.5829279709569223\n",
      "Iteration: 42000\n",
      "Coef: [ 0.77255627 -0.37689429 -0.11302945  0.31854433 -0.18510211  0.23038261\n",
      " -0.647369   -0.79393589  0.37427792  0.54378121], [-0.08238368]\n",
      "Loss: 0.5829049485295794\n",
      "Iteration: 43000\n",
      "Coef: [ 0.77774876 -0.37757918 -0.11490863  0.3195546  -0.18414961  0.22967021\n",
      " -0.64611266 -0.79708688  0.37672643  0.5439697 ], [-0.0822134]\n",
      "Loss: 0.5828838967807886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 44000\n",
      "Coef: [ 0.78271604 -0.37824621 -0.11671846  0.32052017 -0.18323534  0.22896855\n",
      " -0.644889   -0.80009596  0.3790699   0.5441469 ], [-0.08204903]\n",
      "Loss: 0.5828646447463524\n",
      "Iteration: 45000\n",
      "Coef: [ 0.78746868 -0.37889502 -0.11846097  0.32144329 -0.1823579   0.22827945\n",
      " -0.64369905 -0.8029704   0.38131306  0.54431379], [-0.08189046]\n",
      "Loss: 0.5828470369816737\n",
      "Iteration: 46000\n",
      "Coef: [ 0.7920167  -0.37952536 -0.12013819  0.32232604 -0.18151592  0.22760435\n",
      " -0.64254353 -0.805717    0.38346042  0.54447125], [-0.08173755]\n",
      "Loss: 0.5828309320166036\n",
      "Iteration: 47000\n",
      "Coef: [ 0.79636955 -0.38013712 -0.12175216  0.3231704  -0.18070806  0.22694442\n",
      " -0.64142283 -0.80834214  0.38551626  0.54462005], [-0.08159016]\n",
      "Loss: 0.5828162009896128\n",
      "Iteration: 48000\n",
      "Coef: [ 0.80053619 -0.38073027 -0.12330486  0.32397822 -0.17993302  0.22630056\n",
      " -0.64033714 -0.8108518   0.38748468  0.54476087], [-0.08144815]\n",
      "Loss: 0.5828027264359437\n",
      "Iteration: 49000\n",
      "Coef: [ 0.80452511 -0.38130487 -0.12479827  0.32475123 -0.17918953  0.22567347\n",
      " -0.6392864  -0.81325161  0.38936956  0.54489432], [-0.08131137]\n",
      "Loss: 0.5827904012087047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.80834062, -0.38186051, -0.12623295,  0.32549035, -0.17847705,\n",
       "         0.22506424, -0.63827142, -0.81554462,  0.39117285,  0.54502082]),\n",
       " array([-0.08117981]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.gradient_descent_logistic(0.01, 50000,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7081122062168309"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = logit.predict(z_standardize(X))\n",
    "(Y_hat == Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5fb7d63cd0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logit.loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
